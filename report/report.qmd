---
title: Predicting Loan Level Mortgage Loss
subtitle: An exploration of machine learning methods
author: Chris Walker
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
include-in-header: 
  - text: |
      \usepackage{fontspec}
      \setmainfont{Calibri}
      \setsansfont{Calibri}
---

```{r}
#| include: false

knitr::opts_chunk$set(
  fig.align = "center",
  fig.width = 3,
  fig.height = 3,
  echo = FALSE
)

library(tidyverse)
library(tidymodels)

source(here::here("src/functions.R"))

plot_color <- "#F28E2B"

exclude <-
  readr::read_rds(
    here::here("data/complete_data.Rds")
  ) |>
  dplyr::mutate(
    exclude = dti == 999 |  cltv == 999 | fico == 9999 | one_borrower == 999
  ) |>
  dplyr::reframe(
    pct = scales::label_percent(0.1)(mean(exclude)),
    n = sum(exclude),
    total = dplyr::n()
  )

base_rec <-
  readr::read_rds(
    here::here("data/base_rec.Rds")
  )

stability <-
  readr::read_csv(
    here::here("data/stability.csv")
  )

performance <-
  readr::read_csv(
    here::here("data/performance.csv")
  )

specs <-
  readr::read_rds(
    here::here("data/specs.Rds")
  )

train_test <-
  readr::read_rds(
    here::here("data/train_test.Rds")
  )
```

# Introduction

This document describes the motivation, data, estimation, and validation of a credit loss model which compares non-parametric machine learning techniques to predict realized mortgage-level losses in basis points (Bps) of origination UPB. Moreover, this model is designed to estimate gross loss which represents risk to the financial system and not any one party (Freddie Mac, mortgage insurers, etc). The motivations for this project are twofold:

1.  Study the relationship between common loan application details (credit score, debt-to-income ratio, loan-to-value ratio, etc) and mortgage losses.
2.  Evaluate a range of machine learning methods which can be used to model complex regression problems, many of which contain non-linear relationships.

Obtaining good estimates of expected gross loss in a stress scenario is useful for healthy management of the financial system, particularly when losses are estimated early in the mortgage life cycle such as the time of application. It is worth noting that extreme mortgage losses are rare and are generally contained to stressed economic periods, including the great recession period. Thus, this model aims to predict loan-level loss for a mortgage given a stress scenario.

First, a few definitions:

```{r}
tibble::tribble(
  ~ Term, ~ Abbreviation, ~ Definition,
  "debt-to-income", "DTI", "Ratio of debt to a person's income",
  "combined loan-to-value", "CLTV", "Ratio of loan balance to property value",
  "credit score", "FICO", "Representation of credit history risk",
  "basis-points", "Bps", "0.01% of a percent",
  "unpaid balance", "UPB", "Loan balance to be paid",
  "mortgage insurance", "MI", "Insurance paid to offset losses",
  "delinquent", "DQ", "Failure to make payments"
) |>
  knitr::kable(
    caption = "Definitions used in credit modeling"
  )
```

# Data

## Data Source and Exclusions

Data for this project comes courtesy of [Freddie Mac's loan level data repository](https://www.freddiemac.com/research/datasets/sf-loanlevel-dataset). This data repository contains anonymous mortgage data in two primary components; origination data and over time data. Origination data contains information about a loan at its origin. This contains fields like the borrowers' UPB, FICO, LTV, and DTI when the loan was created. Alternatively, the over time data contains monthly summaries of mortgage activity as a borrower pays down their mortgage. This over time data contains information about losses.

This data is generally cleaned and ready for use. However, this section focuses on exclusions made when determining the best model sample for our use case. Exclusions are important because they define the universe for which our model is estimated. Sample selection determines which observations are in our training data and greatly impact the correlation relationships we identify.

Freddie Mac provides data for acquisition vintages ranging from 1999 to 2021. We chose the 2007 vintage for two primary reasons. First, we wanted to simulate a stressed scenario and 2007 represents the height of loss values as shown in the figure below. Second, computational resources limited the amount of data we could use at a single time creating the need to select one vintage.

```{r}
tibble::tibble(
  year = seq(1999, 2021),
  loss = c(
    0.1, 0.1, 0.2, 0.3,
    0.7, 1.4, 6.3, 15.6,
    20.3, 2.3, rep(0, 13)
  )
) |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = year,
      y = loss
    )
  ) +
  ggplot2::geom_line(
    color = plot_color
  ) +
  ggplot2::geom_curve(
    data = tibble::tibble(1),
    arrow = ggplot2::arrow(
      angle = 45,
      length = ggplot2::unit(0.075, "in")
    ),
    ggplot2::aes(
      x = 2014,
      xend = 2008,
      y = 15,
      yend = 20
    )
  ) +
  ggplot2::geom_text(
    data = tibble::tibble(1),
    size = 3,
    ggplot2::aes(
      x = 2014,
      y = 13.5,
      label = "Peak losses in 2007"
    )
  ) +
  ggplot2::labs(
    x = "Vintage",
    y = "Total Loss (Billions)",
    title = "Total Losses by Vintage",
    subtitle = "Sourced from Freddie Mac Data",
    caption = figure_no("Annualized loss value totals")
  ) +
  theme_plot()
```

We applied the following exclusions 2007 acquisitions in accordance with the pseudocode below. The first two exclusions set the universe for the types of mortgage losses we are interested in modeling. Not all loans in this population experienced a loss. Using the [user guide provided by Freddie Mac](https://www.freddiemac.com/fmac-resources/research/pdf/user_guide.pdf), we restrict to loans which satisfy these conditions:

-   Is a third party sale, short sale or charge off, REO disposition, or whole loan sale

-   Does not have a populated defect settlement date

-   Has all components of the loss calculation present

These `zero_bal_code` and `net_loss` filters ensure a given mortgage experienced at least some losses relevant to our modeling use case and that all components of the loss calculation are present. The loss universe exclusions trim the 2007 vintage data from 821238 observations to `r exclude$total` observations.

```{sql, echo=TRUE, eval=FALSE}
zero_bal_code in ('02', '03', '09', '15')
and net_loss != 0
```

Freddie Mac has denoted special missing values for several predictors we are interested in using in our models. Those are denoted below as filters for `dti`, `cltv`, `fico`, and `one_borrower`. In total these missing value exclusions remove an additional `r exclude$n` observations.

```{sql, echo=TRUE, eval=FALSE}
dti != 999
and cltv != 999
and fico != 9999
and one_borrower != 999
```

Our complete data post-exclusion set contains `r nrow(dplyr::bind_rows(train_test))` records from the 2007 vintage. We split the data into train and test using 75% and 25% of the data, respectively. The train population is used for all model fitting and tuning procedures. We used resampling approaches such as bootstrapping for interim steps (like hyperparameter tuning). Note that the testing data is never used for model fitting, tuning, or interim evaluation. Thus, the model comparison contains no data leakage. Finally, we use the test population for final comparison between models.

```{r}
train_test |>
  dplyr::bind_rows(
    .id = "pop"
  ) |>
  dplyr::count(
    Population = stringr::str_to_title(pop),
    name = "Observations"
  ) |>
  knitr::kable(
    caption = "Train test splits"
  )
```

## Target Variable

A mortgage loss occurs when a borrower fails to make payments and enters delinquent status. The financial system incurs losses when borrowers become delinquent for an extended period of time. We compute loss as risk to the financial system. This means that it is possible to compute the amount of losses incurred by Freddie Mac (for example, losses minus insurance payments) but instead we compute total (or gross) losses incurred by every involved party. Our loss computation follows this general form:

$$
\text{Gross Loss}=\text{Delinquent Interest} + \text{Current UPB} + \text{Expenses} - \text{Net Sale}
$$

Delinquent interest is loan interest which accumulates when a borrower fails to make payments. Similarly, current UPB denotes the loan balance at the time of delinquency. Once Freddie Mac repossesses the property they will attempt to sell the property. When the property finally sells much of the original loss is recouped (and sometimes a profit is made). However, in most cases there is still a gross loss balance which is positive representing loss to the financial system. These losses can be offset by MI assistance, but they are not included in this calculation.

$$
\text{Gross Loss Bps}=10000\times\frac{\text{Gross Loss}}{\text{Origination UPB}}
$$

We divide gross loss values by the original loan balance (origination UPB) and convert them to Bps. This provides us a value representing the percentage of a loan's original value which we might expect to be lost given a delinquency in a stressed economic scenario.

Our target variable has the following distribution:

```{r}
ggplot2::ggplot(
  train_test$train,
  ggplot2::aes(gross_loss)
) +
  ggplot2::geom_density(
    color = plot_color
  ) +
  ggplot2::labs(
    x = "Gross Loss (Bps)",
    y = "Density",
    title = "Distribution of Gross Loss",
    subtitle = "Within training data",
    caption = figure_no("Target variable distribution")
  ) +
  theme_plot() +
  ggplot2::theme(
    axis.text.y = ggplot2::element_blank()
  )
```

## Independent Variables

This model makes use of loan level application characteristics to predict loss in basis points in origination UPB given a stressed economic scenario. While more sophisticated models can be estimated when more data is present (for example, early payment history data) we intentionally constrain this model to application level details to act as an early warning loss model. Fields which are present at the time of application include CLTV, DTI, FICO, occupancy status, property type, loan purpose, loan term, first time home buyer, and geographic region.

CLTV, DTI, and FICO are continuous values. This means that these values are a number which is intended to have a positive or negative relationship with gross loss. For example, as FICO scores increase, we would expect gross loss values to fall. All other predictors are categorical variables which means they indicate one of several options. For example, someone can live in one of four regions (but not more than one at once).

In the figure below we plot the distribution of each continuous predictor within the training sample. Both DTI and FICO are approximately normal. The notable exception is CLTV which has large clusters at 80% and 100%. There is incentive for borrowers to put at least 20% down on their mortgage to avoid monthly mortgage insurance payments. Similarly, the cluster around 100% are borrowers who put very little down on their mortgage but still want to purchase a home.

```{r, fig.width=7}
dplyr::slice_sample(
  train_test$train,
  n = 1000
) |>
  withr::with_seed(
    seed = 12345
  ) |>
  tidyr::pivot_longer(
    c(fico, cltv, dti)
  ) |>
  dplyr::mutate(
    name = make_nicenames(name)
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(value)
  ) +
  ggplot2::geom_density(
    color = plot_color
  ) +
  ggplot2::facet_wrap(
    facets = ggplot2::vars(name),
    scales = "free",
    nrow = 1
  ) +
  ggplot2::labs(
    x = "Value",
    y = "Density",
    title = "Distribution of Continuous Predictors",
    subtitle = "From training data",
    caption = figure_no("Predictor distributions")
  ) +
  theme_plot()
```

Below is a table indicating summary statistics for our three numeric predictors. These are sometimes expanded into splines, but they remain column vectors for this table. All three variables have sensible ranges and distributions and contain no missing values.

```{r}
train_test$train |>
  dplyr::select(
    cltv,
    fico,
    dti
  ) |>
  tidyr::pivot_longer(
    dplyr::everything()
  ) |>
  dplyr::group_by(
    Variable = make_nicenames(name)
   ) |>
  dplyr::reframe(
    name = c(seq(0, 1, 0.25), -1, -2),
    value = c(
      quantile(value, name[name >= 0]),
      round(mean(value)),
      sum(is.na(value))
    ),
    name = dplyr::case_when(
      name == -2 ~ "Missing",
      name == -1 ~ "Mean",
      name == 1 ~ "Max",
      name == 0 ~ "Min",
      .default = glue::glue("{name * 100}th")
    )
  ) |>
  tidyr::pivot_wider() |>
  knitr::kable(
    caption = "Numeric predictor summary"
  )
```

Similarly, the table below indicates the shares for each of the categorical terms. It includes both the estimated terms and the dropped reference categories. Several terms are simple binary flags which indicate whether something is true. For example, FTHB (first time home buyer), One Borrower (whether the loan has a single applicant), and One Unit (whether a property is detached from others).

Other categorical terms belong to a set of outcomes (e.g., *Region:* terms). In subsequent estimations one of these would be dropped and set as the reference category. For example, in an OLS coefficient table *Property: Single Family* would not appear because it is the reference value.

```{r}
base_rec |>
  recipes::step_dummy(
    recipes::all_nominal_predictors(),
    one_hot = TRUE
  ) |>
  recipes::prep() |>
  recipes::bake(
    new_data = train_test$train
  ) |>
  dplyr::select(
    - gross_loss,
    - fico,
    - dti,
    - cltv
  ) |>
  tidyr::pivot_longer(
    dplyr::everything(),
    names_transform = make_nicenames,
    names_to = "Variable"
  ) |>
  dplyr::group_by(Variable) |>
  dplyr::reframe(
    Count = sum(value),
    Share = scales::label_percent(0.1)(mean(value))
  ) |>
  knitr::kable(
    caption = "Categorical variable shares"
  )
```

The shares represent the percentage of the time a category or flag is active within our training sample. Percentages which belong to a super set (e.g., *Property:*) will add to 100%. Other independent flags (e.g., FTHB) simply indicate how often this indicator is active.

## Parameter Stability

Here we perform a parameter stability exercise before leveraging more interesting machine learning methods. This method begins by randomly sampling the training data with replacement 100 times. From there, we estimate linear models and assess the stability of the coefficients. There are no additional transformations or splines applied to the continuous predictors, however the categorical variables are one hot encoded.

This means that the default values do not show up on the visual below. The coefficients measure the marginal increase or decrease over the default values for our categorical variables. For example, the default term is 30 years thus the two terms which show 15 and 20 years represent the marginal increase or decrease in expected loss over the 30 year category, holding all else equal.

```{r, fig.width=4, fig.height=5}
coef_df <-
  stability |>
  dplyr::filter(
    term != "(Intercept)"
  ) |>
  dplyr::mutate(
    term = make_nicenames(term),
    significant = dplyr::if_else(
      p.value < 0.01,
      "Significant",
      "Insignificant"
    )
  )

conf_df <-
  coef_df |>
  dplyr::group_by(term) |>
  dplyr::reframe(
    t_test(estimate)
  )

coef_df |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = estimate,
      y = term,
      color = significant
    )
  ) +
  ggplot2::geom_vline(
    xintercept = 0,
    linetype = "dashed"
  ) +
  ggplot2::geom_point() +
  ggplot2::geom_segment(
    data = conf_df,
    inherit.aes = FALSE,
    arrow = ggplot2::arrow(
      ends = "both",
      angle = 90,
      length = ggplot2::unit(0.1, "cm")
    ),
    ggplot2::aes(
      x = lo,
      xend = hi,
      y = term,
      yend = term
    )
  ) +
  ggplot2::labs(
    title = "Coefficient Estimates",
    subtitle = "Random Resample of Training",
    x = "Estimate",
    y = "Term",
    caption = figure_no("Coefficient stability analysis")
  ) +
  theme_plot() +
  ggthemes::scale_color_tableau(
    direction = -1
  )
```

Most coefficients show relative stability with a tight confidence interval. Notable exceptions include 15 year mortgages, the northeast region, and condos. This means that the expected loss of these terms when compared to the defaults are not meaningfully different. Many of these estimations reveal sign flips and insignificant coefficients. The default categories for these terms are 30 years, the Midwest, and single family homes, respectively. This means we will treat these unstable categories as the defaults.

One other variable which might be unstable is DTI. This term tends to correspond with short term default over long term loss. While its confidence interval is small and it is always significant, the term has a negative coefficient which means we would expect losses to fall with increased DTI, which is counter intuitive.

All things considered, the primary terms for this model will be mortgage term, geographic region, loan purpose, property type, one unit, one borrower, occupancy status, first time home buyer, FICO, and CLTV. We will monitor the importance and direction of DTI across each learning method.

# Learning Methods

## Linear Regression

### Background

Our first modeling method is traditional ordinary-least-squares (OLS) regression, also known as linear regression. This method fits an additive model to the data by finding the coefficients or parameters which minimize the sum of squared residuals (or error) between actual and predicted values. However, for continuous predictors with non-linear relationships, we can improve the fit of the model via piece wise splines. This allows the linear plane to have bends or knots which form a jagged shape that can better fit the data.

Lets begin by discussing a brief theoretical background for linear regression or OLS. Arguably the most fundamental and widely used modeling technique, OLS generates a numeric value (called a coefficient) for each variable in the model. It also generates a constant value (also called the intercept) which is used to correct the overall level of the model predictions. Linear regression makes predictions using the following structure:

$$
\hat{y}_i=\beta_0+\sum_{j=1}^{p}X_{ij}\beta_j
$$ Here we want to get the prediction for observation $i$ denoted as $\hat{y}_i$. The prediction is the intercept (denoted as $\beta_0$) plus the sum of $p$ terms. In this case $p$ refers to the number of predictors. We multiply each predictor for observation $i$ (denoted as $X_{ij}$) by the beta for that term (denoted as $\beta_p$).

OLS attempts to minimize the sum of squared residuals. A residual is simply the difference between the actual value for this observation and the predicted value which we can show as $\hat{y}_i - y_i$. The sum of squared residuals is:

$$
\text{Sum of Squared Residuals}=\sum_{i=1}^n (\hat{y}_i - y_i)^2
$$

The values for $\beta_j$ can take the form of any real number. Generalized models make use of a search process called gradient descent to determine the best values for each parameter. However, for OLS we can actually solve for these values making linear models very quick to estimate! It takes this form:

$$
\beta=(X^TX)^{-1}X^Ty
$$

Note that here we refer to the model estimates as $\beta$. In truth these should be denoted as $\hat{\beta}$ because the coefficients themselves are estimates but we adopt this convention for this report. With our model in hand the last step is to estimate p-values for each term, including the intercept. P-values come from estimating standard errors for each term. We can then use these values to conduct t-tests for each term and assess the statistical significance of each $\beta$. Ideally, p-values are very small because small p-values indicate that there is a good chance that the true coefficients (as opposed to the estimated coefficients) are something other than zero and are meaningful.

### Estimation

To construct our splines we will leverage the `earth` package in R which relates to multivariate adaptive regression splines (MARS). Running MARS over our train population results in one knot recommendation per continuous predictor. They turn out to be 720 for FICO, 80 for CLTV, and 37 for DTI. We reject the DTI knot because of DTIs already small (and somewhat counter intuitive) coefficient. Our OLS model has the following specifications:

```{r}
specs$ols |>
  dplyr::reframe(
    Term = make_nicenames(term),
    Estimate = round(estimate, 4),
    P.Value = scales::label_pvalue()(p.value)
  ) |>
  knitr::kable(
    caption = "Linear regression specifications"
  )
```

Note that we did not transform the dependent variable for our OLS model. While it is common to apply a log transformation on loss values (given their long tail) to ensure the key assumptions for OLS regression including constant variance, an expected residual of zero, linearity, and normality of residuals. Below is a direct comparison of the log transformed and non-transformed models. The log transformed model (left) shows that the residuals have a long tail. The non-transformed model (right) however, does reveal residuals which appear more randomly scattered with consistent tails and no discernible patterns.

```{r, fig.width=6}
transform_df <-
  dplyr::bind_rows(
    "Non-Tranformed" = specs$ols_resid,
    "Log-Transformed" = specs$ols_log_resid,
    .id = "transform"
  )

transform_df |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = .pred,
      y = .resid
    )
  ) +
  ggplot2::geom_point(
    color = plot_color,
    size = 0.5
  ) +
  ggplot2::geom_hline(
    yintercept = 0,
    color = "black",
    linetype = "dashed"
  ) +
  theme_plot() +
  ggplot2::facet_wrap(
    facets = ggplot2::vars(transform),
    scales = "free"
  ) +
  ggplot2::labs(
    x = "Fitted Values",
    y = "Residuals",
    title = "Fitted vs Residuals",
    subtitle = "From testing data",
    caption = figure_no("OLS residual comparison")
  )
  
```

The figure below displays the distributions of each OLS model. We observe that the non-transformed model reveals more normally distributed residuals. This helps with model assumptions and reinforces the validity of test statistics (such as t-values and p-values) from the non-transformed OLS model.

```{r fig.width=6}
transform_df |>
  ggplot2::ggplot(
    ggplot2::aes(.resid)
  ) +
  ggplot2::geom_density(
    color = plot_color
  ) +
  theme_plot() +
  ggplot2::facet_wrap(
    facets = ggplot2::vars(transform),
    scales = "free"
  ) +
  ggplot2::labs(
    x = "Fitted Values",
    y = "Density",
    title = "Residual Distributions",
    subtitle = "From testing data",
    caption = figure_no("OLS residual distributions")
  )
```

## XGBoost

### Background

Extreme Gradient Boost (XGBoost or XGB) comes from a family of gradient boosted models. Specifically, XGBoost uses sequentially estimated decision trees which attempt to correct the residuals (or errors) from all prior trees. XGBoost begins by making a simplistic estimate and takes the average response variable for all train observations. The model then predicts this value for all training observations and computes the first set of residuals. The model then fits a full decision tree trying to predict these residuals using available predictors. If the model has a good understanding of residuals, then we can understand sources of variance in the response!

XGBoost uses something called Similarity Score to determine which variable and at which point along the variable (if it is continuous) produces clusters of data that are the most similar. After all, tree based models aim to bucket similar data together. Sufficiently similar data will get a representative prediction. A similarity score is computed as:

$$
\text{Similarity} = \frac{(\sum{\hat{y}-{y}})^2}{\text{Number of Residuals}+\lambda}
$$

To prevent overfitting, XGBoost controls the impact of each tree with a learning rate and a shrinkage parameter. This reduces the impact of any single tree in the series and works to prevent over fitting or rote memorization of the training data. The trees' predictions are then combined, resulting in a robust ensemble that captures intricate relationships between features and the target variable. This learning method is powerful because:

-   There is no need for splines or variable transformations
-   It is robust to outliers and extreme values
-   The algorithm is widely used and optimized for performance (e.g., multi-threading)

However, it has some drawbacks. For example, it has a tendency to over fit despite controls put in place to prevent over fitting. It also does a poor job with extrapolation like other tree methods. This is because XGBoost assigns a value to data based on buckets. If your data is in a bucket at the top or low end of a variable's range (e.g., FICO \> 780) the tree will assign the same value regardless of how far the observation is from the decision point (FICO 780 in the example here). Finally, XGB still struggles with interpretability like other non-linear methods. While metrics (e.g., frequency is a metric discussed later) help to explain feature importance, the model itself contains too many parameters and decision points to be well understood like a linear regression.

### Estimation

Despite the drawbacks of XGBoost, it remains one of the best *batteries included* machine learning methods. This is because it is nearly as easy to estimate as a linear regression and offers many benefits (e.g., reduced need for feature engineering) right out of the gate. However, all XGB models should be fine-tuned to get the best performance. We used hyperparameter tuning with `tidymodels` to tune three key parameters:

-   `tree_depth`: The number of nodes deep a tree grows.
-   `learn_rate`: The relative impact of each tree.
-   `n_trees`: The number of total trees in the model.

All of these values have a direct relationship with over fitting. However, setting these values too small means that the model will do a poor job of understanding the variance in our response variable. Thus, we tuned these values. The default values and ranges come from `tune` in `tidymodels` as to prevent arbitrary value selection. After tuning we selected values of 6, 0.1, and 20 for `tree_depth`, `learn_rate`, and `n_trees`, respectively.

Using the feature importance metrics from `xgboost` we can get a sense of the contribution from each predictor. Where frequency represents the share of nodes across all trees which use each predictor, gain represents the marginal performance contribution of each predictor. A higher value corresponds to greater importance for both values. FICO is the most important predictor followed by CLTV and DTI as measured by frequency. However, gain produces a different rank ordering which we discuss in a later section.

```{r}
specs$xgb |>
  dplyr::arrange(
    desc(frequency)
  ) |>
  dplyr::reframe(
    Feature = make_nicenames(feature),
    Frequency = scales::label_percent(0.1)(frequency),
    Gain = round(gain, 3)
  ) |>
  knitr::kable(
    caption = "XGBoost feature importance"
  )
```

## Neural Network

### Background

The final model type we explored is a deep learning model estimated in Python using `torch`. A deep learning model is simply a neural network of two or more hidden layers. Models of this type can be built to solve extremely complex learning problems including computer vision, language modeling, and more. However, they can still be applied to more traditional predictive applications like this loss model.

A deep learning model for regression is a series of interconnected processing units. Each unit receives data (inputs) representing features, multiplies them by weights (learned values similar to regression coefficients), and sums them up. This sum is then passed through an activation function. The activation function decides how much influence each unit's calculation has on the final output. Common choices in regression include the ReLU function, which allows positive values to pass through unchanged, and zeroes out negative ones. This adds non-linearity, enabling the model to capture complex relationships.

With many interconnected units across multiple layers, each performing this calculation with its own weights and activation function, a deep learning model builds a complex web of influences. By comparing the model's predictions with actual data and adjusting the weights based on errors (back propagation), the model learns to transform the initial inputs into accurate predictions for a target variable.

### Estimation

Our deep learning model, like XGBoost, needs to be tuned to optimize performance. We tuned a total of three parameters each hoping to better the overall regression capability of the model. The parameters are:

-   `num_hidden`: The number of hidden nodes in each of the three hidden layers

-   `dropout`: The share of randomly zeroed inputs (to prevent over fitting)

-   `active_fn`: The activation function used (ReLU vs LeakyReLU)

Similar to the XGB tune, we utilized default values from `tidymodels` to initialize our tuning. For `num_hidden` we tried 10, 20, and 30 whereas we tried 0% and 20% for the number of randomly dropped inputs. Both of these parameters are designed to control for over fitting of the model. Models with less dropout and more hidden layers will fit the training data well but likely over fit and perform poorly on the testing data.

We also tested ReLU vs LeakyReLU as our activation function. As discussed above, these functions introduce non-linearity into the model to allow for complex relationships. The weight (or coefficient) simply adjusts the angle between the flat and angled portions of the activation functions. ReLU is a very popular choice because it is computationally efficient and simple to implement. The LeakyReLU expands on this idea by adding a slope to the flat portion to prevent exploding or vanishing gradients. Exploding or vanishing gradients are two extremes of the same problem related to the chain rule and convergence failures. After a full gamut of testing, we found that ReLU with 30 hidden nodes and 0% dropout leads to the best performance.

```{r, fig.width=4, fig.height=2}
tibble::tibble(
  x = c(-1, 0, 1),
  ReLU = c(0, 0, 1),
  LeakyReLU = c(-0.1, 0, 1)
) |>
  tidyr::pivot_longer(
    c(ReLU, LeakyReLU)
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = x,
      y = value,
      color = name
    )
  ) +
  ggplot2::geom_line(
    linewidth = 1
  ) +
  theme_plot()  +
  ggplot2::labs(
    title = "Activation Functions",
    x = "X",
    y = "Y",
    caption = figure_no("Activation functions in deep learning")
  ) +
  ggplot2::theme(
    legend.position = "none"
  ) +
  ggplot2::facet_wrap(
    facets = ggplot2::vars(name)
  ) +
  ggthemes::scale_color_tableau(
    direction = -1
  )
```

A deep learning network feeds forward by taking the design matrix, multiplying it with weights (hidden layers), and applying activation functions. In our case, our deep learning model contains three hidden layers and one output layer. It also uses the ReLU function. Assume that *X* is our design matrix and *W* are our weight matrices. We can denote the feed forward in pseudocode as:

``` python
# The design matrix
X = matrix()

# The activation function
ReLU = lambda x: max(0, x)

# Feed forward
for W in layers:
  X = ReLU(X @ W)

# Predictions as a column-vector:
print(X)
```

# Model Comparison

## Feature Importance

We wanted to compare the importance of features across XGB and the OLS model. To do this, we investigated the rank ordering of terms from the XGB model using gain and the absolute t-statistic from an OLS estimated using normalized terms. Note, a better method would be to use Shapley values, however, this was not possible due to computational constraints. More on this in the *Future Considerations* section.

The table below shows the results of this exercise. It is important to remember that because the rank ordering for each model structure comes from different measures which prioritize different outcomes, this ranking comes with limitations. However, it does help us understand how OLS and XGB prioritize these terms.

```{r}
dplyr::bind_rows(
  XGB = dplyr::select(
    specs$xgb,
    feature,
    value = gain
  ),
  OLS = dplyr::select(
    specs$ols_std_coef,
    feature = term,
    value = statistic
  ),
  .id = "name"
) |>
  dplyr::filter(
    feature != "(Intercept)"
  ) |>
  dplyr::group_by(name) |>
  dplyr::arrange(
    desc(abs(value))
  ) |>
  dplyr::reframe(
    name,
    value = make_nicenames(feature),
    Rank = dplyr::row_number()
  ) |>
  tidyr::pivot_wider() |>
  dplyr::mutate(
    "Rank Change (OLS to XGB)" = purrr::map2_dbl(
      OLS, Rank,
      \(x, y) y - which(XGB == x)
    )
  ) |>
  dplyr::select(
    Rank,
    OLS,
    dplyr::contains("("),
    XGB
  ) |>
  knitr::kable(
    caption = "Comparison of feature importance"
  )
```

Many terms remained in the same position while many others moved one or less positions. Terms which moved two or more positions up in importance (when moving from OLS to XGB) include *Property: Planned*, and *DTI*. However, it dropped the importance of *Occupy: Second* and *Purpose: Refi* by two positions.

## Performance

In this section we compare the relative model performance of each model using three metrics. These metrics are aimed at measuring the applicability of each model for different use cases. Specifically, Gini and TMR are designed to measure rank ordering performance whereas RMSE is designed to measure accuracy. Gini is directly related to the receiver-operator-curve or ROC. While ROC is more common in data science, Gini is common in credit risk applications and takes the following form:

$$
\text{Gini}=2\times\text{ROC}-1
$$

Both Gini and ROC measure the model's ability to rank order a population. Thus they place greater emphasis on the model's ability to assign a higher value to riskier observations than a less risky observation regardless of the truthfulness of that actual value. This is in contrast to the square-root-mean-squared-error or RMSE which places more emphasis on accurate model prediction and takes the following form:

$$
\text{RMSE}=\sqrt{\frac{1}{n}\sum{(\hat{y}_i-y_i})^2}
$$

Finially, tail-match-rate or TMR, like Gini, measures a model's ability to rank order. However, Gini measures rank ordering performance across the working range of our response variable. TMR measures the overlap between the highest 5% of actual loss values and 5% predicted values by asking the question "what percentage of the 5% highest loss values are within the 5% highest predicted values?" It takes the following form:

$$
\text{TMR} = \frac{1}{n}\sum I(\hat{y}_i \geq \text{95th percentile of } \hat{y} \text{ and } y_i \geq \text{95th percentile of } y)
$$

Where *n* is:

$$
n=\sum{I(y_i \geq \text{95th percentile of }y)}
$$

In general we would like to maximize Gini and TMR to improve rank ordering while minimizing RMSE to reduce loss. However, in a credit decision setting we would prioritize rank ordering over accuracy. This is because most models are compared to some threshold which decides what is an acceptable amount of risk for an approved application. The specific threshold does not matter so long as those above the threshold are riskier than those below it as indicated by rank ordering performance.

If we leverage Gini as our dominant metric then XGBoost provides the best overall performance. This is meaningful because Gini measures rank ordering performance across the range of the response variable. However, if we use TMR or RMSE the deep learning model estimated using `torch` is the best performer for both train and test. Both models perform better than linear regression (OLS) despite the introduction of splines in our linear model.

```{r}
performance |>
  dplyr::mutate(
    population = make_nicenames(population),
    model = make_nicenames(model),
    gini = round(gini, 3),
    tmr = scales::label_percent(0.1)(tmr),
    rmse = round(rmse)
  ) |>
  dplyr::rename_with(make_nicenames) |>
  knitr::kable(
    caption = "Model performance"
  )
```

While it may seem that we would have difficulty choosing between XGBoost and `torch`, the deep learning model was significantly more time and research intensive to develop. Furthermore, it was much more sensitive during our tuning phase while also taking much longer to converge!

This does not discredit the usefulness of deep learning models. However, it does suggest that relative to XGBoost, deep learning models are more challenging to develop particularly for narrow predictive applications. Likewise, XGBoost and its associated frameworks have made strides in developing useful model constraints. For example, monotonicity controls which allow modelers to specify the type of relationship a variable should have with the response (positive or negative). It is for these reasons that our **champion model is our tuned XGBoost**. We recommend this model type for a wide variety of modeling applications.

# Future Considerations

In this section we denote several considerations for future research. First, we would use more data if we were to restart this research project. This project makes use of the 2007 vintage (peak losses). However, deep learning models require considerable amounts of data to be estimated with confidence. This means that the deep learning model was at somewhat of a disadvantage in this modeling comparison. Thus, we would likely use a larger dataset or include additional vintages before exploring deep learning further. This would lead to better comparisons between deep learning and the other model types.

Second, we performed a crude comparison of feature importance using normalized OLS coefficients versus gain from XGBoost. A better approach would be to use Shapley values package. Shapley values provide feature importance scores for almost any model type making it trivial to compare across model structures. However, when we attempted this we quickly ran out of computational resources for the XGBoost model for even a small subset of Shapley combinations. Thus, we did not include a Shapley comparison in this report.

Finally, we could improve model selection by using a three-way split of our data (especially if we used more observations). This would have divided our data into train, test, and validation. While there was no data leakage between train and test in this model, we did resample train and make hyperparameter decisions about each model using the training data. It would have been better to develop and tune models with train, choose the best tuning configuration using a validation set, and compare across models with test.

# Resources

-   [Linear regression with tidymodels](https://parsnip.tidymodels.org/reference/linear_reg.html)

-   [XGBoost with tidymodels](https://parsnip.tidymodels.org/reference/boost_tree.html)

-   [Deep learning with torch](https://pytorch.org/)

-   [An intro to Shapley values](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)

-   [Freddie Mac's loan level data repository](https://www.freddiemac.com/research/datasets/sf-loanlevel-dataset)