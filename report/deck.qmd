---
title: "Predicting Loan Level Loss: An Exploration of Methods"
subtitle: "Christopher Walker"
format:
  revealjs:
    margin: 0
    logo: "logo.png"
    theme: ["theme.scss", default]
    slide-number: "c/t"
    title-slide-attributes:
      data-background-image: "logo.png"
      data-background-opacity: "0.2"
fig-height: 8
fig-width: 8
---

```{r}
library(tidyverse)
library(tidymodels)

source(here::here("src/functions.R"))

sysfonts::font_add_google("Source Sans 3")

train_test <-
  readr::read_rds(
    here::here("data/train_test.Rds")
  )

stability <-
  readr::read_csv(
    here::here("data/stability.csv")
  )

performance <-
  readr::read_csv(
    here::here("data/performance.csv")
  )
```

## Introduction

This deck describes the development of a model which compares **modeling techniques to predict mortgage losses**. The motivations for this project are twofold:

-   Study the relationship between **loan application details and losses**
-   **Evaluate** a range of **machine learning methods**

Obtaining good estimates of loss is useful for **healthy management of the financial system**, particularly when losses are estimated early in the mortgage life cycle. The output from this model could be used to drive **loan approval decisions** or used internally to **drive risk management targets** and decisions.

## Data Sources

Data for this project comes courtesy of **Freddie Mac's loan level data repository**. This data set contains fields like the borrowers' **UPB, FICO, CLTV, and DTI** when the loan was created. It also includes loan level **loss data** where a loss is incurred if a loan becomes delinquent for an extended period of time.

```{r}
train_test |>
  purrr::pluck("train") |>
  dplyr::slice_head(
    n = 5
  ) |>
  dplyr::reframe(
    fico,
    fthb,
    one_unit,
    occupancy,
    "..." = "...",
    purpose,
    term,
    one_borrower,
    gross_loss = round(gross_loss)
  ) |>
  dplyr::mutate(
    dplyr::across(
      !`...`,
      \(x) dplyr::if_else(
        dplyr::row_number() == 5,
        "...", as.character(x)
      )
    )
  ) |>
  knitr::kable()
```

## Target Variable

::: columns
::: {.column width="50%"}
We compute **loss as risk to the financial system**. This means that it is possible to compute the amount of losses incurred by Freddie Mac. For example, losses minus insurance payments.

Instead, we **computed gross losses incurred by every involved party**. Insurance plans, adjustments, and other compensation can reduce the loss experienced by a single institution.

$$
\text{Gross Loss Bps}=1e4\times\frac{\text{Gross Loss}}{\text{UPB}}
$$
:::

::: {.column width="50%"}
```{r}
train_test |>
  purrr::pluck("train") |>
  ggplot2::ggplot(
    ggplot2::aes(gross_loss)
  ) +
  ggplot2::geom_density(
    linewidth = 1
  ) +
  ggplot2::labs(
    x = "Value",
    y = "Density",
    title = "Distribution of Gross Loss",
    subtitle = "Using training dataset"
  ) +
  theme_plot() +
  ggplot2::theme(
    axis.text.y = ggplot2::element_blank()
  )
```
:::
:::

## Independent Variables

::: columns
::: {.column width="50%"}
The independent variables for this model include **several continuous and categorical factors**. **FICO, CLTV, and DTI** are our continuous predictors whereas geographic region, number of borrowers, occupancy type, property type, and loan purpose are categorical.

The visual on the right shows **distributions of the continuous predictors**.
:::

::: {.column width="50%"}
```{r}
train_test |>
  purrr::pluck("train") |>
  dplyr::select(
    FICO = fico,
    DTI = dti,
    CLTV = cltv
  ) |>
  tidyr::pivot_longer(
    dplyr::everything()
  ) |>
  ggplot2::ggplot(
    ggplot2::aes(value)
  ) +
  ggplot2::facet_wrap(
    ggplot2::vars(name),
    scales = "free",
    ncol = 1
  ) +
  ggplot2::geom_density(
    linewidth = 1
  ) +
  ggplot2::labs(
    x = "Value",
    y = "Density",
    title = "Continuous Predictors",
    subtitle = "Using training dataset"
  ) +
  theme_plot() +
  ggplot2::theme(
    axis.text.y = ggplot2::element_blank()
  )
```
:::
:::

## Predictor Stability

::: columns
::: {.column width="50%"}
We utilized a bootstrapping approach to **validate the usefulness of different predictors**. Specifically, we resampled our training data 25 times with replacement.

Using each new sample, we **fit a linear model** and assess the **stability of each coefficient**. **Most predictors are stable** and reveal very tight confidence intervals. However, **Term 25yr, Region: NE, and Property: Condo** all prove to be **unstable** and will be excluded from all modeling trials.
:::

::: {.column width="50%"}
```{r}
coef_df <-
  stability |>
  dplyr::filter(
    term != "(Intercept)"
  ) |>
  dplyr::mutate(
    term = make_nicenames(term),
    significant = dplyr::if_else(
      p.value < 0.01,
      "Significant",
      "Insignificant"
    )
  )

conf_df <-
  coef_df |>
  dplyr::group_by(term) |>
  dplyr::reframe(
    t_test(estimate)
  )

coef_df |>
  ggplot2::ggplot(
    ggplot2::aes(
      x = estimate,
      y = term,
      color = significant
    )
  ) +
  ggplot2::geom_vline(
    xintercept = 0,
    linetype = "dashed",
    linewidth = 1
  ) +
  ggplot2::geom_point(
    size = 5
  ) +
  ggplot2::geom_segment(
    data = conf_df,
    inherit.aes = FALSE,
    linewidth = 1,
    arrow = ggplot2::arrow(
      ends = "both",
      angle = 90,
      length = ggplot2::unit(0.25, "cm")
    ),
    ggplot2::aes(
      x = lo,
      xend = hi,
      y = term,
      yend = term
    )
  ) +
  ggplot2::labs(
    x = "Estimate",
    y = "Term",
    caption = "Brackets show a 95% CI t-test",
    title = "Coefficient Stability",
    subtitle = "by Model Predictor"
  ) +
  theme_plot() +
  ggplot2::scale_color_manual(
    values = c("#003057", "#B3A369")
  )
```
:::
:::

## Modeling Approaches

::: columns
::: {.column width="50%"}
Predicting loss is a complex and sometimes non-linear modeling problem. As a result, we **tested a wide variety of modeling algorithms** of increasing complexity.

The simplest approach **(OLS) acts as our control parametric method**. We utilized **MARS** to develop linear splines to improve the fit. From there, we move into machine learning methods. These methods are designed to capture more **robust non-linear relationships**.
:::

::: {.column width="50%"}
The interface for these approaches is a mix of `tidymodels` in R and `torch` in Python. The approaches tried so far are:

-   Linear Regression (OLS)

-   XGBoost (XGB)

-   Multilayer Perception (MLP)

-   PyTorch (Torch)
:::
:::

## Perfomance Metrics

::: columns
::: {.column width="50%"}
### Tail Match Rate

Tail match rate (TMR) is a measure which measures the percentage of the **highest 5% of gross loss values** which are also in the **5% highest predicted gross loss values**. It is a measure of **rank ordering**. We want to maximize this value.

### Root Mean Squared Error

Root mean squared error (RMSE) measures the **square root of the average squared residual**. It is a measure of **predictive accuracy**. We want to minimize this value.

:::
::: {.column width="50%"}
### Gini

Similar to TMR, Gini **is a measure of rank ordering**. Gini first identifies the **highest 5% of gross loss values** (referred to as tail values). Gini is close to **0.00 if tail values are evenly distributed throughout** the range of predicted values. Alternatively, if values are **clustered near the top** of the predicted range **Gini approaches 1.00**. We want to maximize this value.
:::
:::

## Hyperparameter Tuning

::: columns
::: {.column width="50%"}
All machine learning methods require tuning. A **hyperparameter** is a model parameter which exists outside of the data itself. Examples for XGB include the **max tree depth and learning rate**.

All three non-OLS methods went through a tuning process with a final set of parameters. These parameters were **selected through a bootstrapping process** optimizing for **RMSE, Gini, and TMR performance**.
:::

::: {.column width="50%"}
```{r}
params <- purrr::map(dials, as.character)

tibble::tribble(
  ~ Model, ~ Parameter, ~ Value,
  "XGB", "Max Depth", params$xgb_depth,
  "XGB", "Learn Rate", params$xgb_rate,
  "XGB", "Trees", params$xgb_rounds,
  "MLP", "Epochs", params$mlp_epochs,
  "MLP", "Hidden Nodes", params$mlp_nodes,
  "Torch", "Activation", params$torch_fun,
  "Torch", "Hidden Nodes", params$torch_hidden,
  "Torch", "Dropout Rate", params$torch_dropout
) |>
  knitr::kable()
```
:::
:::



## Model Performance

::: columns
::: {.column width="40%"}
The table on the right shows the **performance results by model type**. The ordering of models by performance remains the same between train and test.

**Gini is the dominant metric on this table** where **XGBoost performs the best**. However, from a TMR perspective Torch performs well. **XGBoost is my preferred method** given ease of development.
:::

::: {.column width="60%"}
```{r}
performance |>
  dplyr::mutate(
    
    # Prepare titles
    dplyr::across(
      c(population, model),
      make_nicenames
    ),
    
    gini = round(gini, 2),
    tmr = scales::label_percent(0.1)(tmr),
    rmse = round(rmse)

  ) |>
  dplyr::rename_with(
    make_nicenames
  ) |>
  knitr::kable()
  
```
:::
:::

## What's Left?

We plan to add **additional discussion** around two key areas in the final report:

-   An overview of questions related **modeling theory**:

    -   How is an XGBoost model fit?

    -   What advantages to tree based models have on tabular data?

    -   What makes a model a deep learning model?

-   A robust discussion of **model interpretability**, especially for XGBoost given it is our recommended model methodology.


